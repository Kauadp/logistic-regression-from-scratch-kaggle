# ğŸ“Š RegressÃ£o LogÃ­stica do Zero â€” PrediÃ§Ã£o no Kaggle (Sem Bibliotecas de ML)

![Python](https://img.shields.io/badge/Python-3.x-blue?logo=python)
![NumPy](https://img.shields.io/badge/NumPy-Linear%20Algebra-orange?logo=numpy)
![No ML Libraries](https://img.shields.io/badge/Machine%20Learning%20Libraries-None-red)
![Kaggle](https://img.shields.io/badge/Kaggle-Score%200.74401-20beff?logo=kaggle)
![Status](https://img.shields.io/badge/Status-Completed-success)

--- 


## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um pipeline completo de Machine Learning do zero, sem uso de bibliotecas de ML como **scikit-learn**, para resolver um problema de classificaÃ§Ã£o binÃ¡ria do Kaggle utilizando RegressÃ£o LogÃ­stica otimizada por Gradiente Ascendente.

O objetivo principal do projeto Ã© consolidar o entendimento **matemÃ¡tico** dos modelos de classificaÃ§Ã£o, indo alÃ©m do uso de bibliotecas prontas e implementando cada etapa do pipeline manualmente
 - PrÃ©-processamento de dados
 - PadronizaÃ§Ã£o de variÃ¡veis
 - AnÃ¡lise de Componentes Principais (PCA)
 - EstimaÃ§Ã£o por MÃ¡xima VerossimilhanÃ§a (MLE)
 - OtimizaÃ§Ã£o por mÃ©todos de gradiente

Score final no Kaggle (public leaderboard): **0.74401**

---

## ğŸ§  Conceitos Implementados

O projeto cobre tanto teoria estatÃ­stica quanto otimizaÃ§Ã£o numÃ©rica:

 - AnÃ¡lise ExploratÃ³ria de Dados (EDA)
 - Tratamento de valores ausentes (missing values)
 - PadronizaÃ§Ã£o (z-score)
 - PCA implementado manualmente via matriz de correlaÃ§Ã£o e autovalores
 - RegressÃ£o LogÃ­stica implementada do zero
 - MÃ¡xima VerossimilhanÃ§a para dados Bernoulli
 - Gradiente Ascendente para otimizaÃ§Ã£o
 - GeraÃ§Ã£o de prediÃ§Ãµes e arquivo de submissÃ£o para o Kaggle

Nenhuma biblioteca de Machine Learning foi utilizada para o treinamento do modelo.

---

## ğŸ§® Modelos MatemÃ¡ticos Utilizados

### ğŸ“ˆ RegressÃ£o Linear â€” ImputaÃ§Ã£o da VariÃ¡vel *Age*

Para imputar valores ausentes da variÃ¡vel **Age**, foi utilizado um modelo de **RegressÃ£o Linear**, assumindo uma relaÃ§Ã£o aproximadamente linear entre a idade e outras variÃ¡veis explicativas do conjunto de dados.

Dado:

- Matriz de variÃ¡veis explicativas: X âˆˆ â„â¿Ë£áµ–  
- Vetor resposta: y âˆˆ â„â¿ (idade)

Modelo:

y = XÎ² + Îµ  
onde: Îµ ~ N(0, ÏƒÂ² I)

O estimador de MÃ­nimos Quadrados OrdinÃ¡rios (MQO) Ã© dado por:

Î²Ì‚ = (Xáµ€ X)â»Â¹ Xáµ€ y

As idades ausentes sÃ£o imputadas por:

Å· = X Î²Ì‚

Dessa forma, a imputaÃ§Ã£o preserva relaÃ§Ãµes estatÃ­sticas entre as variÃ¡veis, em vez de utilizar valores constantes como mÃ©dia ou mediana.

---

### ğŸ“Š RegressÃ£o LogÃ­stica â€” ClassificaÃ§Ã£o BinÃ¡ria

O modelo de RegressÃ£o LogÃ­stica foi utilizado para resolver o problema de **classificaÃ§Ã£o binÃ¡ria** do Kaggle, estimando a probabilidade de ocorrÃªncia da classe positiva.

Dado:

- Matriz de variÃ¡veis: X âˆˆ â„â¿Ë£áµ–  
- Vetor de parÃ¢metros: Î² âˆˆ â„áµ–

Preditor linear:

z = XÎ²

FunÃ§Ã£o sigmoide:

p = Ïƒ(z) = 1 / (1 + e^(âˆ’z))

onde páµ¢ = P(Yáµ¢ = 1 | Xáµ¢)

Assumindo:

Yáµ¢ ~ Bernoulli(páµ¢)

A log-verossimilhanÃ§a do modelo Ã©:

â„“(Î²) = yáµ€ log(p) + (1 âˆ’ y)áµ€ log(1 âˆ’ p)

O gradiente da log-verossimilhanÃ§a Ã©:

âˆ‡â„“(Î²) = Xáµ€ (y âˆ’ p)

A estimaÃ§Ã£o dos parÃ¢metros Ã© feita por **Gradiente Ascendente**:

Î²^(t+1) = Î²^(t) + Î± Xáµ€ (y âˆ’ p)

onde Î± Ã© a taxa de aprendizado (*learning rate*).

Esse procedimento Ã© iterado atÃ© convergÃªncia, maximizando a log-verossimilhanÃ§a do modelo.


---

## âš™ï¸ Tecnologias Utilizadas
 
 - Python
 - NumPy
 - Pandas
 - Matplotlib (para visualizaÃ§Ãµes bÃ¡sicas)

Sem uso de bibliotecas de Machine Learning para treinamento.

---

## ğŸ¯ Resultados

 - Score pÃºblico no Kaggle: 0.74401
 - Modelo: RegressÃ£o LogÃ­stica otimizada por Gradiente Ascendente (maximizaÃ§Ã£o da log-verossimilhanÃ§a)
 - ReduÃ§Ã£o de dimensionalidade: PCA implementado manualmente


## ğŸ‘¤ Autor

**KauÃ£ Dias**  
Estudante de EstatÃ­stica e entusiasta de CiÃªncia de Dados

- ğŸ™ GitHub: [github.com/Kauadp](https://github.com/Kauadp)  
- ğŸ”— LinkedIn: [linkedin.com/in/kauad](https://www.linkedin.com/in/kauad/)



## ğŸ“š ReferÃªncias TeÃ³ricas

Os conceitos estatÃ­sticos, probabilÃ­sticos e algÃ©bricos utilizados neste projeto foram estudados a partir das seguintes referÃªncias:

[1] James, G.; Witten, D.; Hastie, T.; Tibshirani, R.  
**An Introduction to Statistical Learning (ISLR)** â€” Springer.  
https://www.statlearning.com/  
â†’ RegressÃ£o linear, regressÃ£o logÃ­stica, viÃ©sâ€“variÃ¢ncia, classificaÃ§Ã£o.

[2] Stewart, J.  
**CÃ¡lculo â€” Volumes 1 e 2** â€” Cengage Learning.  
https://www.cengage.com.br/livro/calculo-volume-1/  
â†’ Derivadas, gradiente, otimizaÃ§Ã£o e fundamentos para mÃ©todos iterativos.

[3] Lay, D. C.; Lay, S. R.; McDonald, J. J.  
**Ãlgebra Linear e Suas AplicaÃ§Ãµes** â€” Pearson.  
https://www.pearson.com/en-us/subject-catalog/p/linear-algebra-and-its-applications/P200000006472  
â†’ Matrizes, autovalores, autovetores, base para PCA e regressÃ£o matricial.

[4] Boulos, P.  
**Geometria AnalÃ­tica: Um Tratamento Vetorial** â€” McGraw-Hill.  
https://www.grupogen.com.br/livro/geometria-analitica-um-tratamento-vetorial-paulo-boulos  
â†’ InterpretaÃ§Ã£o geomÃ©trica de vetores, projeÃ§Ãµes e espaÃ§os lineares.

[5] Cochran, W. G.  
**Sampling Techniques (Elementos de Amostragem)** â€” Wiley.  
https://www.wiley.com/en-us/Sampling+Techniques%2C+3rd+Edition-p-9780471162407  
â†’ Fundamentos de amostragem, inferÃªncia e variabilidade dos estimadores.

[6] Ross, S. M.  
**A First Course in Probability (Probabilidade)** â€” Pearson.  
https://www.pearson.com/en-us/subject-catalog/p/a-first-course-in-probability/P200000005405  
â†’ VariÃ¡veis aleatÃ³rias, distribuiÃ§Ãµes, Bernoulli e modelos probabilÃ­sticos.

[7] Morettin, P. A.; Bussab, W. O.  
**EstatÃ­stica BÃ¡sica** â€” Saraiva.  
https://www.grupogen.com.br/livro/estatistica-basica-pedro-morettin-wilton-bussab  
â†’ EstatÃ­stica descritiva, inferÃªncia, estimaÃ§Ã£o e testes de hipÃ³teses.